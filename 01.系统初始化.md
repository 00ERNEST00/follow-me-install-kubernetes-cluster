<!-- toc -->

tags: system-env

# 系统初始化

## 集群机器

+ kube-node1：172.27.129.105
+ kube-node2：172.27.129.111
+ kube-node3：172.27.129.112

vagrant 目录下提供了 Vagrantfile 文件，可以使用 vagrant 和 virtualbox 创建对应的三台虚机：

``` bash
$ cd vagrant
$ vagrant up
```

本着测试的目的，etcd 集群、kubernetes master、kubernetes node 均使用这三台机器。

## 主机名

修改每台机器的 `/etc/hosts` 文件，添加主机名和 IP 的对应关系：

``` bash
$ grep kube-node /etc/hosts
172.27.129.105 kube-node1	kube-node1
172.27.129.111 kube-node2	kube-node2
172.27.129.112 kube-node3	kube-node3
```

## 运行账户

``` bash
[root@localhost ~]# useradd -m k8s
[root@localhost ~]# visudo
[root@test-105 ~]# grep '%wheel.*NOPASSWD: ALL' /etc/sudoers
%wheel	ALL=(ALL)	NOPASSWD: ALL
[root@localhost ~]# gpasswd -a k8s wheel
[root@localhost ~]# echo 'PATH=/opt/k8s/bin:$PATH:$HOME/bin:$JAVA_HOME/bin' >>/root/.bashrc
[k8s@localhost ~]$ echo 'PATH=/opt/k8s/bin:$PATH:$HOME/bin:$JAVA_HOME/bin' >>~/.bashrc
```

## docker 账户和配置

``` bash
[root@kube-node1 ~]# useradd -m docker
[root@kube-node1 ~]# gpasswd -a k8s docker
[root@kube-node1 ~]# mkdir -p  /etc/docker/
[root@kube-node1 ~]# cat /etc/docker/daemon.json
{
    "registry-mirrors": ["https://hub-mirror.c.163.com", "https://docker.mirrors.ustc.edu.cn"],
    "insecure-registries": ["docker02:35000"],
    "max-concurrent-downloads": 20
}
```

## 无秘钥登录

``` bash
[k8s@kube-node1 k8s]$ ssh-keygen -t rsa
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node1
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node2
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node3

[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node1
[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node2
[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node3
```

## 安装依赖包

``` bash
[root@kube-node1 ~]# yum install -y epel-release
[root@localhost ~]# yum install -y vim go conntrack ipvsadm ipset jq sysstat curl iptables # ipvs 依赖 ipset
```

## 关闭防火墙

``` bash
[root@kube-node1 ~]# systemctl stop firewalld
[root@kube-node1 ~]# systemctl disable firewalld
[root@kube-node1 ~]# sudo iptables -P FORWARD ACCEPT
```

## 创建目录

``` bash
[root@localhost ~]# mkdir -p /opt/k8s/bin
[root@localhost ~]# chown -R k8s /opt/k8s

[root@kube-node1 ~]# sudo mkdir -p /etc/kubernetes/ssl
[root@kube-node1 ~]# chown -R k8s /etc/kubernetes

[root@kube-node1 ~]# mkdir -p /etc/etcd/ssl
[root@kube-node1 ~]# chown -R k8s /etc/etcd/ssl

[root@kube-node1 ~]# mkdir -p /var/lib/etcd && chown -R k8s /etc/etcd/ssl
```

## 集群环境变量

后续的部署步骤将使用下面定义的全局环境变量，根据**自己的机器、网络情况**修改：

``` bash
#!/usr/bin/bash

# 生成 EncryptionConfig 所需的加密 key
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

# 最好使用 主机未用的网段 来定义服务网段和 Pod 网段

# 服务网段 (Service CIDR），部署前路由不可达，部署后集群内使用IP:Port可达
SERVICE_CIDR="10.254.0.0/16"

# POD 网段 (Cluster CIDR），部署前路由不可达，**部署后**路由可达(flanneld保证)
CLUSTER_CIDR="172.30.0.0/16"

# 服务端口范围 (NodePort Range)
export NODE_PORT_RANGE="8400-9000"

# 集群所有机器 IP
export NODE_IPS=(172.27.129.105 172.27.129.111 172.27.129.112)

# 集群各 IP 对应的 主机名
export NODE_NAMES=(kube-node1 kube-node2 kube-node3)

# kube-apiserver 节点 IP
export MASTER_NODE=172.27.129.105

# kube-apiserver https 地址
export KUBE_APISERVER="https://${MASTER_NODE}:6443"

# etcd 集群服务地址列表
export ETCD_ENDPOINTS="https://172.27.129.105:2379,https://172.27.129.111:2379,https://172.27.129.112:2379"

# etcd 集群间通信的IP和端口
export ETCD_NODES="kube-node1=https://172.27.129.105:2380,kube-node2=https://172.27.129.111:2380,kube-node3=https://172.27.129.112:2380"

# flanneld 网络配置前缀
export FLANNEL_ETCD_PREFIX="/kubernetes/network"

# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)
export CLUSTER_KUBERNETES_SVC_IP="10.254.0.1"

# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)
export CLUSTER_DNS_SVC_IP="10.254.0.2"

# 集群 DNS 域名
export CLUSTER_DNS_DOMAIN="cluster.local."

# 将二进制目录 /opt/k8s/bin 加入到 PATH 中
export PATH=/opt/k8s/bin:$PATH
```
+ 打包后的变量定义见 [environment.sh](https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/environment.sh)，后续部署时会**提示导入**该脚本；

## 分发集群环境变量定义脚本

把全局变量定义脚本拷贝到**所有**机器的 `/opt/k8s/bin` 目录：

``` bash
$ cp environment.sh /opt/k8s/bin
$ chmod +x /opt/k8s/bin/*
```