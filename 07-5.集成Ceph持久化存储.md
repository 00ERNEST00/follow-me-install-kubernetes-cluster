# 07-5.集成 Ceph 持久化存储

## Ceph 集群情况

机器列表：

    172.27.128.100  mon.m7-common-dfs04
    172.27.128.101  mon.m7-common-dfs03
    172.27.128.102  mon.m7-common-dfs02
    172.27.128.103  mon.m7-common-dfs01

组件部署情况：

    Deploy：m7-common-dfs04
    Mon：m7-common-dfs04 m7-common-dfs02 m7-common-dfs03
    Mgr：m7-common-dfs04 m7-common-dfs02 m7-common-dfs03
    Mds：m7-common-dfs04 m7-common-dfs02 m7-common-dfs03
    OSD：
        0-2：mon.m7-common-dfs04
        3-5：mon.m7-common-dfs03
        6-8：mon.m7-common-dfs02
        9-11：mon.m7-common-dfs01
    RGW：mon.m7-common-dfs04

## 安装 Ceph 客户端工具

需要在使用 Ceph 的每个 K8S 节点上安装 Ceph 客户端工具。

创建 yum 源配置文件：

``` bash
sudo yum install -y epel-release
cat << "EOM" > /etc/yum.repos.d/ceph.repo
[ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-luminous/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-luminous/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-luminous/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
EOM
```
+ 注意：ceph repo 的版本**需要与 ceph 集群版本一致**，如上面配置的是 luminous 版本源。

安装 Ceph 客户端工具：

``` bash
yum clean all && yum update
yum install -y ceph-common
```

安装的命令行工具列表：

``` bash
$ rpm -ql ceph-common|grep bin
/usr/bin/ceph
/usr/bin/ceph-authtool
/usr/bin/ceph-brag
/usr/bin/ceph-conf
/usr/bin/ceph-dencoder
/usr/bin/ceph-post-file
/usr/bin/ceph-rbdnamer
/usr/bin/ceph-syn
/usr/bin/rados
/usr/bin/rbd
```

## 挂载 CephFS 

创建本地挂载目录，如 devops：

``` bash
sudo mkdir -p /etc/ceph /mnt/cephfs/k8s/devops
```

创建 secret 文件：

``` bash
sudo scp root@172.27.128.100:/etc/ceph/ceph.client.admin.keyring /etc/ceph/
sudo awk '/key = / {print $3}' /etc/ceph/ceph.client.admin.keyring >/etc/ceph/ceph-admin.secret
```

挂载 CephFS 根目录 /k8s/：

``` bash
sudo mount -t ceph 172.27.128.100:6789,172.27.128.101:6789,172.27.128.102:6789:/k8s/  /mnt/cephfs/k8s/devops -o name=admin,secretfile=/etc/ceph/ceph-admin.secret,_netdev,noatime
``` 
+ 需要指定多个 MDS 地址，以达到高可用和容错的目的。

在 CephFS 根目录 /k8s/ 中创建集群专用的数据目录，如 devops:

``` bash
mkdir /k8s/devops
```

将创建的集群专用 CephFS 目录（/k8s/devops）挂载到本地目录（/mnt/cephfs/k8s/devops）：

``` bash
sudo umount /mnt/cephfs/k8s/devops
sudo mount -t ceph 172.27.128.100:6789,172.27.128.101:6789,172.27.128.102:6789:/k8s/devops  /mnt/cephfs/k8s/devops -o name=admin,secretfile=/etc/ceph/ceph-admin.secret,_netdev,noatime
```

在 /etc/fstab 中添加一行开启自动挂载记录：

``` bash
172.27.128.100:6789:/k8s/devops/ /mnt/cephfs/k8s/devops ceph name=admin,secretfile=/etc/ceph/ceph-admin.secret,_netdev,noatime 0 0
```

## 创建 ceph admin secret

``` bash
[root@m7-demo-136001 k8s]# cd /opt/k8s
[root@m7-demo-136001 k8s]# scp root@172.27.128.100:/etc/ceph/ceph.client.admin.keyring /etc/ceph/

[root@m7-demo-136001 k8s]# cat /etc/ceph/ceph.client.admin.keyring
[client.admin]
        key = AQCYLTdbCyxZBhAAbGfK3T2tczjbhhbR0UWq1w==

[root@m7-demo-136001 k8s]# Secret=$(awk '/key = / {print $3}' /etc/ceph/ceph.client.admin.keyring | base64)
[root@m7-demo-136001 k8s]# cat > ceph-secret-admin.yaml <<EOF
apiVersion: v1
kind: Secret
type: kubernetes.io/rbd
metadata:
  name: ceph-secret-admin
data:
  key: $Secret
EOF
```


注意，如果是通过命令行创建 secret-admin，则**不需要**对key进行base64编码：

  $ kubectl create secret generic ceph-secret-admin --from-literal=key='AQCYLTdbCyxZBhAAbGfK3T2tczjbhhbR0UWq1w==' --namespace=kube-system --type=kubernetes.io/rbd

``` bash
[root@m7-demo-136001 k8s]#  kubectl create -f ceph-secret-admin.yaml
secret "ceph-secret-admin" created
```

注意：PVC 只能使用所在命名空间的 rbd secret。所以上面的定义的 ceph-secret-admin **只能供** default 命名空间的 PVC 使用。其它命名空间的 PVC 如果需要用该 StorageClass，则**需要在所在命名空间重新定义该 Secret**；

## 创建 StorageClass

``` bash
[root@m7-demo-136001 k8s]# cat >ceph-rbd-storage-class.yaml <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
provisioner: kubernetes.io/rbd
parameters:
    monitors: 172.27.128.100:6789,172.27.128.101:6789,172.27.128.102:6789
    adminId: admin
    adminSecretName: ceph-secret-admin
    adminSecretNamespace: "default"
    pool: rbd
    userId: admin
    userSecretName: ceph-secret-admin
EOF
```
+ 额外的缺省参数： imageFormat、imageFeatures；
+ imageFormat 默认值为 1。如果指定为 2，则需要 v3.11 以上内核才行，支持 Clone 等高级特性；
+ imageFeatures 默认值为空，即不启用任何特性。目前支持 layering 特性；

``` bash
[root@m7-demo-136001 k8s]# kubectl create -f ceph-rbd-storage-class.yaml
storageclass "ceph" created
```

查看创建的 StorageClass：

``` bash
$ kubectl get storageclass
NAME      PROVISIONER
ceph      kubernetes.io/rbd
```

通过将 StorageClass 对象的 storageclass.kubernetes.io/is-default-class annotations 设置为 true，可将该 StorageClass 设置为 Default StorageClass：

``` bash
$ kubectl patch storageclass ceph -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
storageclass "ceph" patched
$ kubectl get storageclass
NAME             PROVISIONER
ceph (default)   kubernetes.io/rbd
```

## 创建使用 StorageClass 的 PVC

``` bash
[root@m7-demo-136001 k8s]# cat >ceph-pvc-storageClass.json <<EOF
{
  "kind": "PersistentVolumeClaim",
  "apiVersion": "v1",
  "metadata": {
    "name": "pvc-test-claim"
  },
  "spec": {
    "accessModes": [
      "ReadWriteOnce"
    ],
    "resources": {
      "requests": {
        "storage": "1Gi"
      }
    },
    "storageClassName": "ceph"
  }
}
EOF

[root@m7-demo-136001 k8s]# kubectl create -f ceph-pvc-storageClass.json
persistentvolumeclaim "pvc-test-claim" created

[root@m7-demo-136001 k8s]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
pvc-0335828d-90b7-11e8-b43c-0cc47a2af650   1Gi        RWO            Delete           Bound     default/pvc-test-claim   ceph                     20m
[root@m7-demo-136001 k8s]# kubectl describe pv pvc-0335828d-90b7-11e8-b43c-0cc47a2af650
Name:            pvc-0335828d-90b7-11e8-b43c-0cc47a2af650
Labels:          <none>
Annotations:     kubernetes.io/createdby=rbd-dynamic-provisioner
                 pv.kubernetes.io/bound-by-controller=yes
                 pv.kubernetes.io/provisioned-by=kubernetes.io/rbd
StorageClass:    ceph
Status:          Bound
Claim:           default/pvc-test-claim
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          RBD (a Rados Block Device mount on the host that shares a pod's lifetime)
    CephMonitors:  [172.27.128.100:6789 172.27.128.101:6789 172.27.128.102:6789]
    RBDImage:      kubernetes-dynamic-pvc-e1726fb7-90ba-11e8-9ba5-0cc47a2af650
    FSType:
    RBDPool:       rbd
    RadosUser:     admin
    Keyring:       /etc/ceph/keyring
    SecretRef:     &{ceph-secret-admin}
    ReadOnly:      false
Events:            <none>
```

## 创建使用 PVC 的 Pod

``` bash
[root@m7-demo-136001 k8s]# cat > ceph-pv-test.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: ceph-pv-test
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: ceph-vol
      mountPath: /mnt/rbd
      readOnly: false
  volumes:
  - name: ceph-vol
    persistentVolumeClaim:
      claimName: pvc-test-claim
EOF

[root@m7-demo-136001 k8s]# kubectl create -f ceph-pv-test.yaml
pod "ceph-pv-test" created

[root@m7-demo-136001 k8s]# kubectl describe pods ceph-pv-test
Name:         ceph-pv-test
Namespace:    default
Node:         m7-demo-136001/172.27.136.1
Start Time:   Thu, 26 Jul 2018 18:25:48 +0800
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           172.30.24.43
Containers:
  busybox:
    Container ID:  docker://c6c9f851cefe21a301caf10e4c18c68851c379d83477628560fa3c4006593c5b
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:d21b79794850b4b15d8d332b451d95351d14c951542942a816eea69c9e04b240
    Port:          <none>
    Command:
      sleep
      3600
    State:          Running
      Started:      Thu, 26 Jul 2018 18:25:54 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /mnt/rbd from ceph-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4km88 (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  ceph-vol:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc-test-claim
    ReadOnly:   false
  default-token-4km88:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-4km88
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age                From                       Message
  ----    ------                 ----               ----                       -------
  Normal  SuccessfulMountVolume  51s                kubelet, m7-demo-136001  MountVolume.SetUp succeeded for volume "default-token-4km88"
  Normal  Scheduled              50s                default-scheduler          Successfully assigned ceph-pv-test to m7-demo-136001
  Normal  SuccessfulMountVolume  50s (x2 over 50s)  kubelet, m7-demo-136001  MountVolume.SetUp succeeded for volume "pvc-0335828d-90b7-11e8-b43c-0cc47a2af650"
  Normal  Pulling                49s                kubelet, m7-demo-136001  pulling image "busybox"
  Normal  Pulled                 46s                kubelet, m7-demo-136001  Successfully pulled image "busybox"
  Normal  Created                46s                kubelet, m7-demo-136001  Created container
  Normal  Started                46s                kubelet, m7-demo-136001  Started container
```